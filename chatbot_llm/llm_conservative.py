#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLM Conservador - Respostas Menores e Coerentes
Autor: Andr√© Kroetz Berger
Disciplina: Intelig√™ncia Artificial 2025/2
Universidade La Salle
"""

from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

class ConservativeLLM:
    def __init__(self):
        """Inicializa o LLM com configura√ß√µes conservadoras"""
        print("Carregando modelo GPT-2 para vers√£o conservadora...")
        
        # Carrega modelo e tokenizer
        self.model_name = "gpt2"
        self.tokenizer = GPT2Tokenizer.from_pretrained(self.model_name)
        self.model = GPT2LMHeadModel.from_pretrained(self.model_name)
        
        # Adiciona token de padding se n√£o existir
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # Configura√ß√µes conservadoras
        self.generation_config = {
            'temperature': 0.3,          # Baixa temperatura = mais determin√≠stico
            'max_length': 50,            # Respostas curtas
            'top_p': 0.8,               # Vocabul√°rio mais restrito
            'repetition_penalty': 1.2,   # Evita repeti√ß√µes
            'do_sample': True,
            'pad_token_id': self.tokenizer.eos_token_id
        }
        
        print("Modelo carregado com configura√ß√µes conservadoras!")
        self.print_configuration()
    
    def print_configuration(self):
        """Imprime as configura√ß√µes utilizadas"""
        print("\nüéõÔ∏è CONFIGURA√á√ïES CONSERVADORAS:")
        print(f"Temperature: {self.generation_config['temperature']} (baixa - mais determin√≠stico)")
        print(f"Max Length: {self.generation_config['max_length']} tokens (respostas curtas)")
        print(f"Top-p: {self.generation_config['top_p']} (vocabul√°rio restrito)")
        print(f"Repetition Penalty: {self.generation_config['repetition_penalty']} (evita repeti√ß√µes)")
        print("-" * 60)
    
    def generate_response(self, prompt):
        """Gera resposta usando configura√ß√µes conservadoras"""
        try:
            # Codifica o prompt
            inputs = self.tokenizer.encode(prompt, return_tensors='pt')
            
            # Gera resposta
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    **self.generation_config
                )
            
            # Decodifica apenas a parte nova (sem o prompt original)
            response = self.tokenizer.decode(
                outputs[0][len(inputs[0]):], 
                skip_special_tokens=True
            ).strip()
            
            return response if response else "I understand, but I need more context to provide a helpful response."
            
        except Exception as e:
            return f"Error generating response: {e}"
    
    def start_conversation(self):
        """Inicia conversa com o usu√°rio"""
        print("=" * 60)
        print("ü§ñ LLM CONSERVADOR - RESPOSTAS COERENTES E CONCISAS ü§ñ")
        print("=" * 60)
        print("Ol√°! Sou um assistente com configura√ß√µes conservadoras.")
        print("Forne√ßo respostas curtas, diretas e coerentes.")
        print("Digite 'sair' para encerrar.")
        print("-" * 60)
        
        while True:
            try:
                user_input = input("\nüë§ Voc√™: ")
                
                if user_input.lower() in ['sair', 'exit', 'quit', 'bye']:
                    print("\nü§ñ Bot: Obrigado pela conversa! At√© mais!")
                    break
                
                if user_input.strip() == "":
                    print("\nü§ñ Bot: Por favor, digite sua pergunta.")
                    continue
                
                # Gera resposta
                print("\nü§ñ Bot: ", end="", flush=True)
                response = self.generate_response(user_input)
                print(response)
                
            except KeyboardInterrupt:
                print("\n\nConversa interrompida. At√© mais!")
                break
            except Exception as e:
                print(f"\n‚ùå Erro: {e}")

def main():
    """Fun√ß√£o principal"""
    try:
        # Cria inst√¢ncia do LLM conservador
        conservative_llm = ConservativeLLM()
        
        # Inicia conversa
        conservative_llm.start_conversation()
        
    except Exception as e:
        print(f"‚ùå Erro ao inicializar LLM: {e}")
        print("Verifique se o PyTorch e Transformers est√£o instalados.")

if __name__ == "__main__":
    main()
